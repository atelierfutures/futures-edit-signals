{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266b7543",
   "metadata": {},
   "source": [
    "\n",
    "# The Futures Edit — Signals (Merged Master Notebook)\n",
    "*Rebuild date:* **2025-09-16 22:09**  \n",
    "This notebook merges your existing Signals pipeline with new connectors and a normalized output.\n",
    "It’s **drop-in ready** for your web app (Chart.js) with consistent CSV/Parquet/JSONL exports.\n",
    "\n",
    "### What’s included\n",
    "- **Shared utils** (retry, parsing, hashing, save helpers)\n",
    "- **Config** (YAML or inline) + **Secrets** (env vars)\n",
    "- **Connectors**: LS:N Global sectors, Reddit, Google Trends, TikTok (optional, Playwright), RSS news feeds, Substack\n",
    "- **(Optional) Legacy keywords scraper hook** — paste your old keywords scraping cell into the provided hook and it will auto-merge\n",
    "- **Normalization & dedupe**\n",
    "- **Light heuristics for summaries** (top sources, frequent tokens, simple trend deltas) for executive overview\n",
    "- **Exports**: `/data/signals_all.*` + derived summary CSVs\n",
    "\n",
    "> You can paste this whole notebook over your existing one, or open alongside and migrate cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c54f7f",
   "metadata": {},
   "source": [
    "## Setup — Install dependencies (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339768a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "pip -q install praw==7.7.1 feedparser==6.0.10 pytrends==4.9.2 beautifulsoup4==4.12.3 lxml==5.2.2 html5lib==1.1 pandas==2.2.2 numpy==2.1.1 requests==2.32.3 pyyaml==6.0.2 tqdm==4.66.5 tenacity==9.0.0 python-dateutil==2.9.0.post0 rapidfuzz==3.9.7\n",
    "# Optional: TikTok via Playwright (best-effort). Comment if not needed.\n",
    "pip -q install playwright==1.47.0\n",
    "python -m playwright install chromium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bc704",
   "metadata": {},
   "source": [
    "## Imports & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json, yaml, time, random, hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Optional TikTok\n",
    "try:\n",
    "    from playwright.sync_api import sync_playwright\n",
    "    HAS_PLAYWRIGHT = True\n",
    "except Exception:\n",
    "    HAS_PLAYWRIGHT = False\n",
    "\n",
    "OUT_DIR = Path(\"./data\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def now_iso():\n",
    "    return datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "\n",
    "def to_iso(dt):\n",
    "    if dt is None or (isinstance(dt, float) and np.isnan(dt)):\n",
    "        return None\n",
    "    if isinstance(dt, datetime):\n",
    "        return dt.replace(microsecond=0).isoformat() + \"Z\"\n",
    "    try:\n",
    "        return dateparser.parse(str(dt)).replace(microsecond=0).isoformat() + \"Z\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def hash_id(*parts):\n",
    "    s = \"::\".join([str(p) for p in parts if p is not None])\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "\n",
    "def normalize_record(source_type, source_name, title, url, published_at=None, summary=None, tags=None, extra=None):\n",
    "    return {\n",
    "        \"id\": hash_id(source_type, source_name, url, title),\n",
    "        \"source_type\": source_type,\n",
    "        \"source_name\": source_name,\n",
    "        \"title\": (title or \"\").strip(),\n",
    "        \"url\": url,\n",
    "        \"published_at\": to_iso(published_at),\n",
    "        \"summary\": (summary or \"\").strip() if summary else None,\n",
    "        \"tags\": tags or [],\n",
    "        \"extra\": extra or {},\n",
    "        \"ingested_at\": now_iso(),\n",
    "    }\n",
    "\n",
    "def save_df(df, stem):\n",
    "    stem = re.sub(r\"[^a-zA-Z0-9_\\-]\", \"_\", stem)\n",
    "    csv_path = OUT_DIR / f\"{stem}.csv\"\n",
    "    parquet_path = OUT_DIR / f\"{stem}.parquet\"\n",
    "    jsonl_path = OUT_DIR / f\"{stem}.jsonl\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(json.dumps(row.to_dict(), ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved:\\n- {csv_path}\\n- {parquet_path}\\n- {jsonl_path}\")\n",
    "    return csv_path, parquet_path, jsonl_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7536dc0b",
   "metadata": {},
   "source": [
    "## Secrets — set env vars after running this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9399ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reddit\n",
    "os.environ[\"REDDIT_CLIENT_ID\"] = os.getenv(\"REDDIT_CLIENT_ID\", \"\")\n",
    "os.environ[\"REDDIT_CLIENT_SECRET\"] = os.getenv(\"REDDIT_CLIENT_SECRET\", \"\")\n",
    "os.environ[\"REDDIT_USER_AGENT\"] = os.getenv(\"REDDIT_USER_AGENT\", \"signals/1.0 by yourname\")\n",
    "\n",
    "# LS:N Global (if you have access; scraper works anonymously for headlines but may be partial)\n",
    "os.environ[\"LSNG_EMAIL\"] = os.getenv(\"LSNG_EMAIL\", \"\")\n",
    "os.environ[\"LSNG_PASSWORD\"] = os.getenv(\"LSNG_PASSWORD\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ff4fd0",
   "metadata": {},
   "source": [
    "## CONFIG — edit here or upload `sources.config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "DEFAULT_CONFIG = {\n",
    "  \"lsng\": {\n",
    "    \"sectors\": [\n",
    "      {\"name\": \"Beauty\", \"url\": \"https://www.lsnglobal.com/sectors/beauty\"},\n",
    "      {\"name\": \"Fashion\", \"url\": \"https://www.lsnglobal.com/sectors/fashion\"},\n",
    "      {\"name\": \"Health & Wellness\", \"url\": \"https://www.lsnglobal.com/sectors/health-wellness\"},\n",
    "    ],\n",
    "    \"max_pages\": 2\n",
    "  },\n",
    "  \"reddit\": {\n",
    "    \"subreddits\": [\"femalefashionadvice\",\"SkincareAddiction\",\"AsianBeauty\",\"MakeupAddiction\",\"malefashion\",\"fashion\",\"fragrance\",\"streetwear\",\"trendanalysis\",\"DataIsBeautiful\"],\n",
    "    \"query\": \"signals OR trend OR 'future of' OR beauty OR fashion OR wellness\",\n",
    "    \"time_filter\": \"week\",\n",
    "    \"limit\": 100\n",
    "  },\n",
    "  \"gtrends\": {\n",
    "    \"geo\": \"ES\",\n",
    "    \"keywords\": [\"beauty trends\",\"fashion trends\",\"wellness trends\",\"dopamine dressing\",\"skin cycling\",\"quiet luxury\"],\n",
    "    \"tz\": 120,\n",
    "    \"related_queries\": True\n",
    "  },\n",
    "  \"tiktok\": {\n",
    "    \"hashtags\": [\"beauty\",\"fashion\",\"wellness\",\"trendtok\",\"skincare\",\"outfitideas\"],\n",
    "    \"max_per_hashtag\": 20\n",
    "  },\n",
    "  \"rss\": {\n",
    "    \"feeds\": [\n",
    "      \"https://www.voguebusiness.com/feed/rss\",\n",
    "      \"https://www.businessoffashion.com/feed\",\n",
    "      \"https://www.glossy.co/feed/\",\n",
    "      \"https://www.beautyindependent.com/feed/\",\n",
    "      \"https://www.highsnobiety.com/rss\",\n",
    "      \"https://www.thecut.com/fashion/rss/index.xml\"\n",
    "    ],\n",
    "    \"max_per_feed\": 50\n",
    "  },\n",
    "  \"substack\": {\n",
    "    \"feeds\": [\n",
    "      \"https://linernotes.substack.com/feed\",\n",
    "      \"https://piratewires.substack.com/feed\",\n",
    "      \"https://readmaximums.substack.com/feed\"\n",
    "    ],\n",
    "    \"max_per_feed\": 50\n",
    "  }\n",
    "}\n",
    "\n",
    "CONFIG_PATH = Path(\"sources.config.yaml\")\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    print(\"Loaded config from sources.config.yaml\")\n",
    "else:\n",
    "    cfg = DEFAULT_CONFIG\n",
    "    with open(CONFIG_PATH, \"w\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "    print(\"Wrote default config → sources.config.yaml\")\n",
    "\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659783a",
   "metadata": {},
   "source": [
    "## Collect — LS:N Global (Beauty, Fashion, Health & Wellness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=6))\n",
    "def get(url, session=None, **kwargs):\n",
    "    s = session or requests.Session()\n",
    "    headers = kwargs.pop(\"headers\", {})\n",
    "    headers.setdefault(\"User-Agent\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\")\n",
    "    r = s.get(url, headers=headers, timeout=30, **kwargs)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def parse_lsng_sector_page(html, sector_name, base_url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    for a in soup.select(\"a\"):\n",
    "        title = (a.get_text() or \"\").strip()\n",
    "        href = a.get(\"href\")\n",
    "        if not href or not title: \n",
    "            continue\n",
    "        if len(title.split()) < 3:\n",
    "            continue\n",
    "        if not href.startswith(\"http\"):\n",
    "            href = requests.compat.urljoin(base_url, href)\n",
    "        card = a.find_parent([\"article\",\"div\",\"li\"]) or a\n",
    "        summary = None\n",
    "        date_txt = None\n",
    "        txt = \" \".join(card.get_text(\" \").split())\n",
    "        m = re.search(r\"(\\b\\d{1,2}\\s+[A-Za-z]{3,}\\s+\\d{4}\\b|\\b[A-Za-z]{3,}\\s+\\d{1,2},\\s+\\d{4}\\b|\\b\\d{4}-\\d{2}-\\d{2}\\b)\", txt)\n",
    "        if m: date_txt = m.group(1)\n",
    "        ps = card.find_all(\"p\")\n",
    "        if ps:\n",
    "            summary = ps[0].get_text(\" \").strip()[:500]\n",
    "        items.append(normalize_record(\"lsng\", sector_name, title, href, date_txt, summary))\n",
    "    return items\n",
    "\n",
    "def collect_lsng(cfg):\n",
    "    sectors = cfg[\"lsng\"][\"sectors\"]\n",
    "    max_pages = cfg[\"lsng\"].get(\"max_pages\", 1)\n",
    "    all_items = []\n",
    "    session = requests.Session()\n",
    "    for s in sectors:\n",
    "        base = s[\"url\"]\n",
    "        for page in range(1, max_pages+1):\n",
    "            url = base if page == 1 else f\"{base}?page={page}\"\n",
    "            try:\n",
    "                r = get(url, session=session)\n",
    "                items = parse_lsng_sector_page(r.text, s[\"name\"], base)\n",
    "                all_items.extend(items)\n",
    "                time.sleep(random.uniform(0.5, 1.2))\n",
    "            except Exception as e:\n",
    "                print(f\"[LSNG] Error {s['name']} page {page}: {e}\")\n",
    "    return pd.DataFrame(all_items)\n",
    "\n",
    "df_lsng = collect_lsng(cfg)\n",
    "print(\"LS:N rows:\", len(df_lsng))\n",
    "df_lsng.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e63027",
   "metadata": {},
   "source": [
    "## Collect — Reddit (PRAW search across configured subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import praw\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def reddit_client():\n",
    "    cid = os.environ.get(\"REDDIT_CLIENT_ID\")\n",
    "    csec = os.environ.get(\"REDDIT_CLIENT_SECRET\")\n",
    "    ua = os.environ.get(\"REDDIT_USER_AGENT\", \"signals/1.0\")\n",
    "    if not cid or not csec:\n",
    "        raise RuntimeError(\"Set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET in environment\")\n",
    "    return praw.Reddit(client_id=cid, client_secret=csec, user_agent=ua)\n",
    "\n",
    "def collect_reddit(cfg):\n",
    "    rc = reddit_client()\n",
    "    subs = cfg[\"reddit\"][\"subreddits\"]\n",
    "    q = cfg[\"reddit\"][\"query\"]\n",
    "    tfilter = cfg[\"reddit\"].get(\"time_filter\",\"week\")\n",
    "    limit = cfg[\"reddit\"].get(\"limit\",100)\n",
    "    rows = []\n",
    "    for sub in subs:\n",
    "        subreddit = rc.subreddit(sub)\n",
    "        for post in subreddit.search(q, time_filter=tfilter, limit=limit, sort=\"new\"):\n",
    "            rows.append(normalize_record(\n",
    "                \"reddit\", f\"r/{sub}\", post.title,\n",
    "                url=f\"https://www.reddit.com{post.permalink}\",\n",
    "                published_at=dt.utcfromtimestamp(post.created_utc),\n",
    "                summary=(post.selftext or \"\")[:800],\n",
    "                tags=[sub]\n",
    "            ))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "try:\n",
    "    df_reddit = collect_reddit(cfg)\n",
    "    print(\"Reddit rows:\", len(df_reddit))\n",
    "    df_reddit.head(5)\n",
    "except Exception as e:\n",
    "    print(\"Reddit skipped/error:\", e)\n",
    "    df_reddit = pd.DataFrame([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83d446",
   "metadata": {},
   "source": [
    "## Collect — Google Trends (interest over time + related queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df23f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "def collect_gtrends(cfg):\n",
    "    geo = cfg[\"gtrends\"].get(\"geo\",\"\")\n",
    "    tz = cfg[\"gtrends\"].get(\"tz\",0)\n",
    "    kws = cfg[\"gtrends\"][\"keywords\"]\n",
    "    pytrends = TrendReq(hl='en-US', tz=tz)\n",
    "    rows = []\n",
    "    # interest over time (7d window, adjust in config if needed)\n",
    "    pytrends.build_payload(kws, timeframe='now 7-d', geo=geo)\n",
    "    iot = pytrends.interest_over_time()\n",
    "    if not iot.empty:\n",
    "        iot = iot.reset_index().rename(columns={\"date\":\"published_at\"})\n",
    "        for _, r in iot.iterrows():\n",
    "            for kw in kws:\n",
    "                rows.append(normalize_record(\n",
    "                    \"gtrends\",\"interest_over_time\",\n",
    "                    title=f\"{kw} — interest\",\n",
    "                    url=f\"https://trends.google.com/trends/explore?geo={geo}&q={kw}\",\n",
    "                    published_at=r[\"published_at\"],\n",
    "                    summary=f\"Interest score: {r.get(kw, None)}\",\n",
    "                    tags=[kw, geo]\n",
    "                ))\n",
    "    # related queries\n",
    "    if cfg[\"gtrends\"].get(\"related_queries\", True):\n",
    "        rq = pytrends.related_queries()\n",
    "        for kw, data in rq.items():\n",
    "            if not data or \"top\" not in data or data[\"top\"] is None:\n",
    "                continue\n",
    "            for _, row in data[\"top\"].head(25).iterrows():\n",
    "                q = row[\"query\"]\n",
    "                val = row.get(\"value\", None)\n",
    "                rows.append(normalize_record(\n",
    "                    \"gtrends\",\"related_queries\",\n",
    "                    title=f\"Related: {q}\",\n",
    "                    url=f\"https://trends.google.com/trends/explore?geo={geo}&q={kw}\",\n",
    "                    published_at=datetime.utcnow(),\n",
    "                    summary=f\"Parent keyword: {kw}; Score: {val}\",\n",
    "                    tags=[kw,\"related\"]\n",
    "                ))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "try:\n",
    "    df_gtrends = collect_gtrends(cfg)\n",
    "    print(\"Google Trends rows:\", len(df_gtrends))\n",
    "    df_gtrends.head(5)\n",
    "except Exception as e:\n",
    "    print(\"Google Trends skipped/error:\", e)\n",
    "    df_gtrends = pd.DataFrame([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4772b3",
   "metadata": {},
   "source": [
    "## Collect — TikTok (optional, Playwright; best-effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52975a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_tiktok(cfg):\n",
    "    if not HAS_PLAYWRIGHT:\n",
    "        print(\"Playwright not available; skipping TikTok.\")\n",
    "        return pd.DataFrame([])\n",
    "    rows = []\n",
    "    from datetime import datetime as dt\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        context = browser.new_context()\n",
    "        page = context.new_page()\n",
    "        for tag in cfg[\"tiktok\"][\"hashtags\"]:\n",
    "            url = f\"https://www.tiktok.com/tag/{tag}\"\n",
    "            try:\n",
    "                page.goto(url, timeout=60000)\n",
    "                page.wait_for_timeout(5000)\n",
    "                anchors = page.query_selector_all(\"a\")\n",
    "                count = 0\n",
    "                for a in anchors:\n",
    "                    href = a.get_attribute(\"href\") or \"\"\n",
    "                    txt = (a.inner_text() or \"\").strip()\n",
    "                    if \"/video/\" in href and len(txt) > 0:\n",
    "                        rows.append(normalize_record(\n",
    "                            \"tiktok\", f\"#{tag}\", txt[:160], href,\n",
    "                            published_at=dt.utcnow(), tags=[tag]\n",
    "                        ))\n",
    "                        count += 1\n",
    "                        if count >= cfg[\"tiktok\"].get(\"max_per_hashtag\",20):\n",
    "                            break\n",
    "                time.sleep(random.uniform(1.0,2.0))\n",
    "            except Exception as e:\n",
    "                print(f\"TikTok error for #{tag}: {e}\")\n",
    "        context.close()\n",
    "        browser.close()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "try:\n",
    "    df_tiktok = collect_tiktok(cfg)\n",
    "    print(\"TikTok rows:\", len(df_tiktok))\n",
    "    df_tiktok.head(5)\n",
    "except Exception as e:\n",
    "    print(\"TikTok skipped/error:\", e)\n",
    "    df_tiktok = pd.DataFrame([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8802af",
   "metadata": {},
   "source": [
    "## Collect — News feeds (RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be87323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def collect_rss(cfg):\n",
    "    feeds = cfg[\"rss\"][\"feeds\"]\n",
    "    max_per = cfg[\"rss\"].get(\"max_per_feed\",50)\n",
    "    rows = []\n",
    "    for url in feeds:\n",
    "        fp = feedparser.parse(url)\n",
    "        name = urlparse(url).netloc\n",
    "        c = 0\n",
    "        for entry in fp.entries:\n",
    "            title = entry.get(\"title\",\"\")\n",
    "            link = entry.get(\"link\",\"\")\n",
    "            summary = BeautifulSoup(entry.get(\"summary\",\"\") or \"\", \"lxml\").get_text(\" \")\n",
    "            published = entry.get(\"published\") or entry.get(\"updated\")\n",
    "            rows.append(normalize_record(\"rss\", name, title, link, published, summary))\n",
    "            c += 1\n",
    "            if c >= max_per: break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_rss = collect_rss(cfg)\n",
    "print(\"RSS rows:\", len(df_rss))\n",
    "df_rss.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade4e17",
   "metadata": {},
   "source": [
    "## Collect — Substack Newsletters (via RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_substack(cfg):\n",
    "    feeds = cfg[\"substack\"][\"feeds\"]\n",
    "    max_per = cfg[\"substack\"].get(\"max_per_feed\",50)\n",
    "    rows = []\n",
    "    for url in feeds:\n",
    "        fp = feedparser.parse(url)\n",
    "        name = urlparse(url).netloc\n",
    "        c = 0\n",
    "        for entry in fp.entries:\n",
    "            title = entry.get(\"title\",\"\")\n",
    "            link = entry.get(\"link\",\"\")\n",
    "            summary = BeautifulSoup(entry.get(\"summary\",\"\") or \"\", \"lxml\").get_text(\" \")\n",
    "            published = entry.get(\"published\") or entry.get(\"updated\")\n",
    "            rows.append(normalize_record(\"substack\", name, title, link, published, summary))\n",
    "            c += 1\n",
    "            if c >= max_per: break\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_substack = collect_substack(cfg)\n",
    "print(\"Substack rows:\", len(df_substack))\n",
    "df_substack.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5413ec",
   "metadata": {},
   "source": [
    "\n",
    "## (Optional) Legacy — paste your existing **Keywords/Signals scraper** here\n",
    "If you had a previous dataframe (e.g., `df_keywords`), just assign it to `df_legacy`.  \n",
    "Columns don't need to match; we'll normalize minimally via `legacy_to_records()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXAMPLE PLACEHOLDER — replace with your existing logic\n",
    "# df_keywords = ...  # <- your old dataframe\n",
    "\n",
    "try:\n",
    "    df_keywords  # noqa: F821\n",
    "    HAS_LEGACY = True\n",
    "except NameError:\n",
    "    HAS_LEGACY = False\n",
    "\n",
    "def legacy_to_records(df):\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        title = r.get(\"title\") or r.get(\"keyword\") or r.get(\"name\") or \"Untitled\"\n",
    "        url = r.get(\"url\") or r.get(\"link\") or \"\"\n",
    "        published = r.get(\"published\") or r.get(\"date\") or r.get(\"timestamp\")\n",
    "        summary = r.get(\"summary\") or r.get(\"desc\") or \"\"\n",
    "        rows.append(normalize_record(\"legacy\",\"keywords\",title,url,published,summary))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "if HAS_LEGACY:\n",
    "    df_legacy = legacy_to_records(df_keywords)\n",
    "    print(\"Legacy rows:\", len(df_legacy))\n",
    "    display(df_legacy.head(5))\n",
    "else:\n",
    "    df_legacy = pd.DataFrame([])\n",
    "    print(\"No legacy dataframe detected; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88045c",
   "metadata": {},
   "source": [
    "## Normalize, Merge & Dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frames = [df for df in [df_lsng, df_reddit, df_gtrends, df_tiktok, df_rss, df_substack, df_legacy] if isinstance(df, pd.DataFrame) and not df.empty]\n",
    "if frames:\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "else:\n",
    "    df_all = pd.DataFrame([], columns=[\"id\",\"source_type\",\"source_name\",\"title\",\"url\",\"published_at\",\"summary\",\"tags\",\"extra\",\"ingested_at\"])\n",
    "\n",
    "# Basic clean\n",
    "if not df_all.empty:\n",
    "    df_all[\"title\"] = df_all[\"title\"].fillna(\"\")\n",
    "    df_all[\"url\"] = df_all[\"url\"].fillna(\"\")\n",
    "    df_all[\"published_at\"] = df_all[\"published_at\"].fillna(\"\")\n",
    "    df_all[\"summary\"] = df_all[\"summary\"].fillna(\"\")\n",
    "\n",
    "# Dedupe by (url, title) to reduce noise\n",
    "if not df_all.empty:\n",
    "    df_all = df_all.sort_values(by=[\"published_at\",\"ingested_at\"], ascending=False)\n",
    "    df_all = df_all.drop_duplicates(subset=[\"url\",\"title\"], keep=\"first\")\n",
    "\n",
    "print(\"Total normalized rows:\", len(df_all))\n",
    "display(df_all.head(20))\n",
    "save_df(df_all, \"signals_all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8781b9",
   "metadata": {},
   "source": [
    "## Executive Summary (light heuristics; no external AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33258df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_dt(s):\n",
    "    try:\n",
    "        return dateparser.parse(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def summarise(df):\n",
    "    res = {}\n",
    "    if df.empty:\n",
    "        return {\"note\":\"No data to summarise.\"}\n",
    "    # Window: last 7 days vs prior 7\n",
    "    df[\"dt\"] = df[\"published_at\"].apply(safe_dt)\n",
    "    cutoff = datetime.utcnow() - timedelta(days=7)\n",
    "    cur = df[df[\"dt\"] >= cutoff]\n",
    "    prev = df[(df[\"dt\"] < cutoff) & (df[\"dt\"] >= cutoff - timedelta(days=7))]\n",
    "\n",
    "    res[\"items_last_7d\"] = len(cur)\n",
    "    res[\"items_prev_7d\"] = len(prev)\n",
    "    res[\"delta_items_pct\"] = None\n",
    "    if len(prev) > 0:\n",
    "        res[\"delta_items_pct\"] = round((len(cur)-len(prev)) / max(len(prev),1) * 100, 1)\n",
    "\n",
    "    # Top sources last 7 days\n",
    "    top_sources = (cur.groupby([\"source_type\",\"source_name\"])[\"id\"]\n",
    "                     .count().sort_values(ascending=False).head(10).reset_index())\n",
    "    res[\"top_sources\"] = top_sources\n",
    "\n",
    "    # Frequent tokens in titles\n",
    "    toks = []\n",
    "    stop = set(\"the of a an and or for to in on with from by into about your our how what why\".split())\n",
    "    for t in cur[\"title\"].astype(str):\n",
    "        for w in re.findall(r\"[A-Za-z][A-Za-z\\-]+\", t.lower()):\n",
    "            if w not in stop and len(w) > 3:\n",
    "                toks.append(w)\n",
    "    if toks:\n",
    "        s = pd.Series(toks).value_counts().head(30).reset_index()\n",
    "        s.columns = [\"token\",\"count\"]\n",
    "        res[\"top_tokens\"] = s\n",
    "\n",
    "    return res\n",
    "\n",
    "summary = summarise(df_all)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca80a9b",
   "metadata": {},
   "source": [
    "## Export Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if isinstance(summary, dict) and \"note\" not in summary:\n",
    "    top_sources = summary.get(\"top_sources\", pd.DataFrame())\n",
    "    top_tokens = summary.get(\"top_tokens\", pd.DataFrame())\n",
    "    if isinstance(top_sources, pd.DataFrame) and not top_sources.empty:\n",
    "        save_df(top_sources, \"summary_top_sources_7d\")\n",
    "    if isinstance(top_tokens, pd.DataFrame) and not top_tokens.empty:\n",
    "        save_df(top_tokens, \"summary_top_tokens_7d\")\n",
    "else:\n",
    "    print(summary.get(\"note\",\"No summary available.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad4a1e1",
   "metadata": {},
   "source": [
    "## Chart.js Export — minimal payloads (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47956972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build minimal JSONs to fetch directly from your web app if you host them somewhere.\n",
    "def write_json(obj, name):\n",
    "    p = OUT_DIR / f\"{name}.json\"\n",
    "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Wrote\", p)\n",
    "    return p\n",
    "\n",
    "# Example: counts by source_type for last 7 days\n",
    "def counts_by_source_type(df):\n",
    "    df = df.copy()\n",
    "    df[\"dt\"] = pd.to_datetime(df[\"published_at\"], errors=\"coerce\")\n",
    "    cutoff = datetime.utcnow() - timedelta(days=7)\n",
    "    cur = df[df[\"dt\"] >= cutoff]\n",
    "    return (cur.groupby(\"source_type\")[\"id\"].count().sort_values(ascending=False)\n",
    "            .reset_index().rename(columns={\"id\":\"count\"}))\n",
    "\n",
    "if not df_all.empty:\n",
    "    chart_counts = counts_by_source_type(df_all).to_dict(orient=\"records\")\n",
    "    write_json(chart_counts, \"chart_counts_by_source_type_7d\")\n",
    "else:\n",
    "    print(\"No data for chart exports.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7d50f",
   "metadata": {},
   "source": [
    "## Output Manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527dcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manifest = {\n",
    "  \"all_rows_csv\": \"data/signals_all.csv\",\n",
    "  \"all_rows_parquet\": \"data/signals_all.parquet\",\n",
    "  \"all_rows_jsonl\": \"data/signals_all.jsonl\",\n",
    "  \"summary_top_sources_csv\": \"data/summary_top_sources_7d.csv\",\n",
    "  \"summary_top_tokens_csv\": \"data/summary_top_tokens_7d.csv\",\n",
    "  \"chart_counts_by_source_type_7d\": \"data/chart_counts_by_source_type_7d.json\"\n",
    "}\n",
    "write_json(manifest, \"manifest\")\n",
    "manifest\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
