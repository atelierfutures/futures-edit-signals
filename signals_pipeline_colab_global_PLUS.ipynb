{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e35103",
   "metadata": {},
   "source": [
    "\n",
    "# The Futures Edit — Signals Pipeline (GLOBAL · PLUS)\n",
    "- Clear **status** tiers: *emerging / bubbling / mainstream* (percentile-based)\n",
    "- **More keywords**: big multilingual seed lists + **YAKE** auto-extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⬇️ Install dependencies (run once per Colab session)\n",
    "!pip install -q feedparser beautifulsoup4 pandas numpy nltk pytrends praw tqdm python-dateutil pyyaml yake langdetect\n",
    "print(\"✅ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86690dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Mount Google Drive to save outputs\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import os\n",
    "    SAVE_DIR = \"/content/drive/MyDrive/futures_edit_outputs\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(\"Drive mounted. Outputs will also be saved to:\", SAVE_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Drive not mounted (you can ignore this).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07567557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🗂 Create sources_config.yaml (GLOBAL + bigger multilingual seeds)\n",
    "yaml_content = \"\"\"rss:\n",
    "  fashion:\n",
    "    - https://news.google.com/rss/search?q=fashion%20innovation%20OR%20runway%20OR%20couture&hl=en&gl=US&ceid=US:en\n",
    "    - https://news.google.com/rss/search?q=moda%20tendencias%20OR%20pasarela%20OR%20alta%20costura&hl=es&gl=ES&ceid=ES:es\n",
    "    - https://news.google.com/rss/search?q=mode%20tendances%20OR%20haute%20couture%20OR%20d%C3%A9fil%C3%A9&hl=fr&gl=FR&ceid=FR:fr\n",
    "    - https://www.hypebeast.com/feed\n",
    "    - https://www.highsnobiety.com/feed\n",
    "    - https://www.dazeddigital.com/rss\n",
    "    - https://i-d.vice.com/en_uk/rss\n",
    "    - https://theimpression.com/feed/\n",
    "  beauty:\n",
    "    - https://news.google.com/rss/search?q=beauty%20skincare%20innovation%20OR%20makeup%20trend&hl=en&gl=US&ceid=US:en\n",
    "    - https://news.google.com/rss/search?q=belleza%20cuidado%20de%20la%20piel%20tendencia%20maquillaje&hl=es&gl=ES&ceid=ES:es\n",
    "    - https://news.google.com/rss/search?q=beauti%C3%A9%20soin%20de%20la%20peau%20tendance%20maquillage&hl=fr&gl=FR&ceid=FR:fr\n",
    "    - https://www.beautymatter.com/rss.xml\n",
    "    - https://www.glossy.co/feed/\n",
    "    - https://www.allure.com/feed/all/rss\n",
    "    - https://www.refinery29.com/en-us/rss.xml\n",
    "  wellness:\n",
    "    - https://news.google.com/rss/search?q=wellness%20trend%20OR%20longevity%20OR%20recovery%20studio&hl=en&gl=US&ceid=US:en\n",
    "    - https://news.google.com/rss/search?q=bienestar%20tendencias%20salud%20long%20evitad&hl=es&gl=ES&ceid=ES:es\n",
    "    - https://news.google.com/rss/search?q=bien-%C3%AAtre%20tendances%20sant%C3%A9%20long%C3%A9vit%C3%A9&hl=fr&gl=FR&ceid=FR:fr\n",
    "    - https://www.wellandgood.com/feed/\n",
    "    - https://www.mindbodygreen.com/feeds/latest\n",
    "    - https://examine.com/feed/\n",
    "\n",
    "keywords:\n",
    "  fashion:\n",
    "    - quiet luxury\n",
    "    - stealth wealth\n",
    "    - lujo silencioso\n",
    "    - luxe discret\n",
    "    - balletcore\n",
    "    - coquette\n",
    "    - blokette\n",
    "    - mob wife aesthetic\n",
    "    - archival revival\n",
    "    - gorpcore\n",
    "    - techwear\n",
    "    - digital couture\n",
    "    - haute couture numérique\n",
    "    - generative knitwear\n",
    "    - 3D knit\n",
    "    - crochet runway\n",
    "    - upcycled denim\n",
    "    - deadstock\n",
    "    - seasonless drops\n",
    "    - drop model\n",
    "    - rental fashion\n",
    "    - circular design\n",
    "    - luxury resale\n",
    "    - made-to-order\n",
    "    - on-demand production\n",
    "    - mary janes\n",
    "    - ballet flats\n",
    "    - kitten heels\n",
    "    - rosette\n",
    "    - bows\n",
    "    - ribbon\n",
    "  beauty:\n",
    "    - skin cycling\n",
    "    - skin flooding\n",
    "    - skin barrier\n",
    "    - slugging\n",
    "    - glass skin\n",
    "    - jello skin\n",
    "    - mochi skin\n",
    "    - latte makeup\n",
    "    - lip oil\n",
    "    - lip stain\n",
    "    - peptide serum\n",
    "    - copper peptides\n",
    "    - fermented beauty\n",
    "    - microbiome skincare\n",
    "    - mushroom skincare\n",
    "    - adaptogens\n",
    "    - SPF stick\n",
    "    - mineral sunscreen\n",
    "    - LED mask\n",
    "    - red light mask\n",
    "    - microcurrent device\n",
    "    - AI skin analysis\n",
    "    - scalp care\n",
    "    - hair cycling\n",
    "    - fragrance layering\n",
    "    - perfume oils\n",
    "  wellness:\n",
    "    - longevity\n",
    "    - GLP-1\n",
    "    - red light therapy\n",
    "    - cold plunge\n",
    "    - sauna club\n",
    "    - heat therapy\n",
    "    - breathwork\n",
    "    - vagus nerve\n",
    "    - magnesium glycinate\n",
    "    - creatine for women\n",
    "    - protein water\n",
    "    - gut health\n",
    "    - microbiome\n",
    "    - prebiotics\n",
    "    - postbiotics\n",
    "    - electrolytes\n",
    "    - sleep hygiene\n",
    "    - mouth taping\n",
    "    - HRV tracking\n",
    "    - zone 2\n",
    "    - rucking\n",
    "    - pilates\n",
    "    - mobility\n",
    "    - lymphatic drainage\n",
    "\"\"\"\n",
    "with open(\"sources_config.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(yaml_content)\n",
    "print(\"✅ sources_config.yaml created (GLOBAL + extended keywords)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, math, json, yaml, feedparser, pandas as pd, numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import urlparse\n",
    "from dateutil import parser as dateparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional Trends/Reddit\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "except Exception:\n",
    "    TrendReq = None\n",
    "\n",
    "try:\n",
    "    import praw\n",
    "except Exception:\n",
    "    praw = None\n",
    "\n",
    "# Keyword extraction\n",
    "from langdetect import detect\n",
    "import yake\n",
    "\n",
    "with open(\"sources_config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "NOW = datetime.now(timezone.utc)\n",
    "\n",
    "print(\"Categories:\", list(CFG.get(\"rss\", {}).keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82ce7e",
   "metadata": {},
   "source": [
    "## 1) RSS Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text: str) -> str:\n",
    "    soup = BeautifulSoup(text or \"\", \"html.parser\")\n",
    "    return soup.get_text(\" \", strip=True)\n",
    "\n",
    "def parse_date(entry):\n",
    "    for key in (\"published\", \"updated\", \"created\"):\n",
    "        val = entry.get(key)\n",
    "        if val:\n",
    "            try:\n",
    "                return dateparser.parse(val)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "def fetch_rss_feeds(feed_urls, category):\n",
    "    rows = []\n",
    "    for url in tqdm(feed_urls, desc=f\"Fetching {category} feeds\"):\n",
    "        try:\n",
    "            d = feedparser.parse(url)\n",
    "            for e in d.entries:\n",
    "                title = e.get(\"title\", \"\").strip()\n",
    "                link = e.get(\"link\")\n",
    "                summary = clean_html(e.get(\"summary\", \"\"))\n",
    "                dt = parse_date(e)\n",
    "                rows.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"summary\": summary,\n",
    "                    \"published\": dt.isoformat() if dt else None,\n",
    "                    \"source\": urlparse(url).netloc,\n",
    "                    \"category\": category\n",
    "                })\n",
    "        except Exception as ex:\n",
    "            print(f\"Error fetching {url}: {ex}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "frames = []\n",
    "for cat in (\"fashion\",\"beauty\",\"wellness\"):\n",
    "    feeds = CFG.get(\"rss\",{}).get(cat, [])\n",
    "    if feeds:\n",
    "        frames.append(fetch_rss_feeds(feeds, cat))\n",
    "\n",
    "rss_df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "rss_df.to_csv(os.path.join(OUT_DIR, \"rss_raw.csv\"), index=False)\n",
    "print(f\"Fetched {len(rss_df)} items\")\n",
    "rss_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57bf1e",
   "metadata": {},
   "source": [
    "## 2) Scoring + Automatic Keyword Extraction (YAKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fa6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEEDS = CFG.get(\"keywords\", {})\n",
    "\n",
    "def recency_score(published_iso):\n",
    "    if not published_iso:\n",
    "        return 0.5\n",
    "    try:\n",
    "        dt = dateparser.parse(published_iso)\n",
    "        if dt.tzinfo is None:\n",
    "            dt = dt.replace(tzinfo=timezone.utc)\n",
    "        else:\n",
    "            dt = dt.astimezone(timezone.utc)\n",
    "        days = max((NOW - dt).days, 0)\n",
    "        return max(0.0, 1.0 - (days/30.0))\n",
    "    except Exception:\n",
    "        return 0.5\n",
    "\n",
    "def keyword_hit_score(text, seeds):\n",
    "    text_l = (text or \"\").lower()\n",
    "    score = 0\n",
    "    hits = []\n",
    "    for kw in seeds:\n",
    "        if kw.lower() in text_l:\n",
    "            score += 1\n",
    "            hits.append(kw)\n",
    "    return score, hits\n",
    "\n",
    "# Multilingual YAKE extraction\n",
    "def extract_auto_keyword(title, summary, topk=1):\n",
    "    raw = \" \".join([str(title or \"\"), str(summary or \"\")]).strip()\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    # Detect language; fallback to 'en'\n",
    "    try:\n",
    "        lang = detect(raw)\n",
    "        if lang not in (\"en\",\"es\",\"fr\"):\n",
    "            lang = \"en\"\n",
    "    except Exception:\n",
    "        lang = \"en\"\n",
    "    try:\n",
    "        kw = yake.KeywordExtractor(lan=lang, n=3, top=topk)\n",
    "        pairs = kw.extract_keywords(raw)\n",
    "        if not pairs:\n",
    "            return \"\"\n",
    "        # YAKE returns (keyphrase, score) where lower score = more relevant\n",
    "        best = sorted(pairs, key=lambda x: x[1])[0][0]\n",
    "        return best.strip().lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "rows = []\n",
    "for _, row in rss_df.iterrows():\n",
    "    seeds = SEEDS.get(row[\"category\"], [])\n",
    "    kscore_title, hits_t = keyword_hit_score(row[\"title\"], seeds)\n",
    "    kscore_sum, hits_s = keyword_hit_score(row[\"summary\"], seeds)\n",
    "    rec = recency_score(row[\"published\"])\n",
    "    auto_kw = extract_auto_keyword(row[\"title\"], row[\"summary\"], topk=1)\n",
    "    primary_kw = (hits_t + hits_s)[0] if (hits_t or hits_s) else auto_kw\n",
    "    score = 0.7*(kscore_title*1.5 + kscore_sum) + 0.3*(rec*2.0)\n",
    "    rows.append({\n",
    "        **row,\n",
    "        \"keyword_hits\": \"; \".join(sorted(set(hits_t + hits_s))),\n",
    "        \"auto_keyword\": auto_kw,\n",
    "        \"primary_keyword\": primary_kw,\n",
    "        \"recency\": rec,\n",
    "        \"trend_score\": round(score, 3)\n",
    "    })\n",
    "\n",
    "scored_df = pd.DataFrame(rows).sort_values(\"trend_score\", ascending=False)\n",
    "scored_df.to_csv(os.path.join(OUT_DIR, \"rss_scored_plus.csv\"), index=False)\n",
    "scored_df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3be123",
   "metadata": {},
   "source": [
    "## 3) Google Trends Validation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trends_score(terms, geo=\"ES\", timeframe=\"today 3-m\"):\n",
    "    if TrendReq is None:\n",
    "        print(\"pytrends not available; skipping Trends.\")\n",
    "        return {}\n",
    "    pytrends = TrendReq(hl='en-US', tz=360)\n",
    "    results = {}\n",
    "    for term in terms:\n",
    "        if not term:\n",
    "            continue\n",
    "        try:\n",
    "            pytrends.build_payload([term], timeframe=timeframe, geo=geo)\n",
    "            df = pytrends.interest_over_time()\n",
    "            if not df.empty:\n",
    "                tail = df[term].tail(12)\n",
    "                score = (tail.iloc[-1] + 1) / (tail.mean() + 1)\n",
    "                results[term] = round(float(score), 3)\n",
    "        except Exception as e:\n",
    "            results[term] = None\n",
    "    return results\n",
    "\n",
    "top_terms = (\n",
    "    scored_df[\"primary_keyword\"]\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .value_counts()\n",
    "    .head(10)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "trends = trends_score(top_terms, geo=\"ES\", timeframe=\"today 3-m\")\n",
    "trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a09294",
   "metadata": {},
   "source": [
    "## 4) Reddit Mentions (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21292722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_client():\n",
    "    if praw is None:\n",
    "        print(\"praw not available; skipping Reddit.\")\n",
    "        return None\n",
    "    import os\n",
    "    cid = os.getenv(\"REDDIT_CLIENT_ID\", \"\")\n",
    "    csec = os.getenv(\"REDDIT_CLIENT_SECRET\", \"\")\n",
    "    ua = os.getenv(\"REDDIT_USER_AGENT\", \"futures-edit-scout/0.1\")\n",
    "    if not cid or not csec:\n",
    "        print(\"Missing Reddit credentials; skipping Reddit mentions.\")\n",
    "        return None\n",
    "    return praw.Reddit(client_id=cid, client_secret=csec, user_agent=ua)\n",
    "\n",
    "def count_reddit_mentions(terms, subreddits, limit=80):\n",
    "    reddit = reddit_client()\n",
    "    if reddit is None:\n",
    "        return {}\n",
    "    res = {t:0 for t in terms if t}\n",
    "    for sub in subreddits:\n",
    "        try:\n",
    "            for post in reddit.subreddit(sub).new(limit=limit):\n",
    "                text = f\"{post.title} {post.selftext or ''}\".lower()\n",
    "                for t in res.keys():\n",
    "                    if t and t.lower() in text:\n",
    "                        res[t] += 1\n",
    "        except Exception as e:\n",
    "            print(\"Reddit error on r/%s: %s\" % (sub, e))\n",
    "    return res\n",
    "\n",
    "# Default subs if none provided\n",
    "subs_cfg = CFG.get(\"reddit\",{}).get(\"subreddits\",{})\n",
    "default_subs = list(set(sum(subs_cfg.values(), []))) if subs_cfg else [\n",
    "    \"SkincareAddiction\",\"AsianBeauty\",\"MakeUpAddiction\",\"fragrance\",\n",
    "    \"femalefashionadvice\",\"malefashionadvice\",\"techwearclothing\",\n",
    "    \"Supplements\",\"Biohackers\",\"meditation\"\n",
    "]\n",
    "reddit_counts = count_reddit_mentions(top_terms, default_subs, limit=60)\n",
    "reddit_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773dd0b",
   "metadata": {},
   "source": [
    "## 5) Merge Validation + Export (with status tiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "final = scored_df.copy()\n",
    "final[\"trends_momentum\"] = final[\"primary_keyword\"].map(trends).fillna(\"\")\n",
    "final[\"reddit_mentions\"] = final[\"primary_keyword\"].map(reddit_counts).fillna(\"\")\n",
    "\n",
    "# Composite rank\n",
    "if len(final):\n",
    "    rmax = max([as_float(x) for x in final[\"reddit_mentions\"] if str(x)!=\"\"], default=1.0)\n",
    "    final[\"reddit_norm\"] = final[\"reddit_mentions\"].apply(lambda x: as_float(x) / (rmax or 1.0))\n",
    "    final[\"momentum_norm\"] = final[\"trends_momentum\"].apply(as_float)\n",
    "    final[\"signal_rank\"] = (final[\"trend_score\"] + 0.5*final[\"reddit_norm\"] + 0.3*final[\"momentum_norm\"]).round(3)\n",
    "    final = final.sort_values(\"signal_rank\", ascending=False)\n",
    "\n",
    "# Percentile thresholds for status\n",
    "if len(final):\n",
    "    k = final[\"signal_rank\"].astype(float)\n",
    "    p33 = np.percentile(k, 33)\n",
    "    p66 = np.percentile(k, 66)\n",
    "    def status_for(v):\n",
    "        if v >= p66: return \"mainstream\"\n",
    "        if v >= p33: return \"bubbling\"\n",
    "        return \"emerging\"\n",
    "    final[\"status\"] = final[\"signal_rank\"].apply(status_for)\n",
    "else:\n",
    "    p33 = p66 = 0.0\n",
    "    final[\"status\"] = \"\"\n",
    "\n",
    "final_out = final[[\"title\",\"link\",\"summary\",\"published\",\"source\",\"category\",\"keyword_hits\",\"auto_keyword\",\"primary_keyword\",\"trend_score\",\"trends_momentum\",\"reddit_mentions\",\"signal_rank\",\"status\"]]\n",
    "final_out.to_csv(os.path.join(OUT_DIR, \"signals_ranked_plus.csv\"), index=False)\n",
    "print(\"✅ Exported → outputs/signals_ranked_plus.csv\")\n",
    "print(\"Status thresholds → emerging < %.3f ≤ bubbling < %.3f ≤ mainstream\" % (p33, p66))\n",
    "final_out.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e3a5a",
   "metadata": {},
   "source": [
    "## 6) Newsletter Capsules (prefer primary_keyword, fallback to auto_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_capsule(row):\n",
    "    name = row[\"primary_keyword\"] or row[\"auto_keyword\"] or (row[\"title\"][:60] + (\"…\" if len(row[\"title\"])>60 else \"\"))\n",
    "    why = \"This hints at a shift toward \" + (row[\"primary_keyword\"] or row[\"auto_keyword\"] or \"an emerging consumer desire\")\n",
    "    return {\n",
    "        \"Signal Name\": name,\n",
    "        \"Category\": row[\"category\"],\n",
    "        \"Example Link\": row[\"link\"],\n",
    "        \"Why it matters\": why,\n",
    "        \"Score\": row.get(\"signal_rank\",\"\"),\n",
    "        \"Status\": row.get(\"status\",\"\")\n",
    "    }\n",
    "\n",
    "caps_df = pd.DataFrame([to_capsule(r) for _, r in final_out.iterrows()])\n",
    "caps_df.to_csv(os.path.join(OUT_DIR, \"newsletter_capsules_plus.csv\"), index=False)\n",
    "print(\"✅ Exported → outputs/newsletter_capsules_plus.csv\")\n",
    "caps_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a199a",
   "metadata": {},
   "source": [
    "## 7) Copy outputs to Google Drive (if mounted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80197c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, shutil\n",
    "if 'SAVE_DIR' in globals() and os.path.isdir(OUT_DIR):\n",
    "    for f in glob.glob(os.path.join(OUT_DIR, \"*.csv\")):\n",
    "        shutil.copy(f, SAVE_DIR)\n",
    "    print(\"✅ Copied CSVs to\", SAVE_DIR)\n",
    "else:\n",
    "    print(\"Drive not mounted or no outputs yet.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
